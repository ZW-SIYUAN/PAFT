{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_cat_cols(df):\n",
    "  label_dict = {}\n",
    "  for col in df.columns:\n",
    "    sample = np.nan\n",
    "    for i in range(len(df[col])):\n",
    "      if df[col][i]!=df[col][i]: continue\n",
    "      else:\n",
    "        sample = df[col][i]\n",
    "        break\n",
    "    if isinstance(sample, str):\n",
    "      label_dict[col] = {}\n",
    "      le = LabelEncoder()\n",
    "      le.fit(list(df[col]))\n",
    "      labels = list(le.classes_)\n",
    "      for i in range(len(labels)):\n",
    "        label_dict[col][labels[i]] = i\n",
    "\n",
    "  return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "  train = pd.read_csv(f'./data_to_eval/train/{filename}.csv') # train dataset\n",
    "  label_dictionary = encode_cat_cols(train)\n",
    "\n",
    "  test = pd.read_csv(f'./data_to_eval/test/{filename}.csv') # test dataset\n",
    "\n",
    "  generated_great = pd.read_csv(f'./data_to_eval/great/{filename}.csv')\n",
    "\n",
    "  generated_paft = pd.read_csv(f'./data_to_eval/paft/{filename}.csv')\n",
    "\n",
    "  return train, test, generated_great, generated_paft, label_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - ML Efficieny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with generated data and test with ground truth\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "def MLE(train_df, test_df, label_col, label_col_discrete):\n",
    "  X_train = train_df.copy().drop([label_col], axis=1)\n",
    "  y_train = train_df[label_col]\n",
    "  X_test = test_df.copy().drop([label_col], axis=1)\n",
    "  y_test = test_df[label_col]\n",
    "\n",
    "  if label_col_discrete:\n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(X_train, y_train)\n",
    "    print(\"  -> Random Forest\")\n",
    "    y_pred = RF.predict(X_test)\n",
    "    print('       Training Score: ', RF.score(X_train, y_train))\n",
    "    print('       Testing Score: ', RF.score(X_test, y_test))\n",
    "    print('       Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    print(\"  -> Logistic Regression\")\n",
    "    y_pred = LR.predict(X_test)\n",
    "    print('       Training Score: ', LR.score(X_train, y_train))\n",
    "    print('       Testing Score: ', LR.score(X_test, y_test))\n",
    "    print('       Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    NN = MLPClassifier(solver='adam', hidden_layer_sizes=(150, 100, 50), max_iter=300, activation='relu')\n",
    "    NN.fit(X_train, y_train)\n",
    "    print(\"  -> Neural Network\")\n",
    "    y_pred = NN.predict(X_test)\n",
    "    print('       Training Score: ', NN.score(X_train, y_train))\n",
    "    print('       Testing Score: ', NN.score(X_test, y_test))\n",
    "    print('       Accuracy:', accuracy_score(y_test, y_pred))\n",
    "  else:\n",
    "    RF = RandomForestRegressor()\n",
    "    RF.fit(X_train, y_train)\n",
    "    print(\"  -> Random Forest\")\n",
    "    y_pred = RF.predict(X_test)\n",
    "    print('       Training Score: ', RF.score(X_train, y_train))\n",
    "    print('       Testing Score: ', RF.score(X_test, y_test))\n",
    "    print('       Accuracy (r2 score):', r2_score(y_test, y_pred))\n",
    "    print('       Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "    print('       Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_test, y_pred))\n",
    "\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    print(\"  -> Linear Regression\")\n",
    "    y_pred = LR.predict(X_test)\n",
    "    print('       Training Score: ', LR.score(X_train, y_train))\n",
    "    print('       Testing Score: ', LR.score(X_test, y_test))\n",
    "    print('       Accuracy (r2 score):', r2_score(y_test, y_pred))\n",
    "    print('       Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "    print('       Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_test, y_pred))\n",
    "\n",
    "    NN = MLPRegressor(solver='adam', hidden_layer_sizes=(150, 100, 50), max_iter=300, activation='relu')\n",
    "    NN.fit(X_train, y_train)\n",
    "    print(\"  -> Neural Network\")\n",
    "    y_pred = NN.predict(X_test)\n",
    "    print('       Training Score: ', NN.score(X_train, y_train))\n",
    "    print('       Testing Score: ', NN.score(X_test, y_test))\n",
    "    print('       Accuracy:', r2_score(y_test, y_pred))\n",
    "    print('       Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "    print('       Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with ground truth + random data (as different as possible), then test generated data to see if its real/fake\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def Discriminator(real_data_train, real_data_test, generated_data):\n",
    "  real_test = real_data_test.copy()\n",
    "  real_test['truth'] = 1\n",
    "  generated_test = generated_data.copy()\n",
    "  generated_test['truth'] = 0\n",
    "  generated_test = generated_test.sample(len(real_test))\n",
    "\n",
    "  X_test = shuffle(pd.concat([generated_test, real_test])).reset_index(drop=True)\n",
    "  y_test = X_test['truth']\n",
    "  X_test = X_test.drop(['truth'], axis=1)\n",
    "\n",
    "  real_train = real_data_train.copy()\n",
    "  real_train['truth'] = 1\n",
    "  random = {}\n",
    "  float_cols = []\n",
    "  for col in list(real_data_train.columns):\n",
    "    random[col] = np.random.choice(real_data_train[col], len(real_data_train))\n",
    "  random = pd.DataFrame(random)\n",
    "  for col in float_cols:\n",
    "    random[col] = random[col].astype(float)\n",
    "  random['truth'] = 0\n",
    "\n",
    "  X_train = shuffle(pd.concat([real_train, random])).reset_index(drop=True)\n",
    "  y_train = X_train['truth']\n",
    "  X_train = X_train.drop(['truth'], axis=1)\n",
    "\n",
    "  RF = RandomForestClassifier()\n",
    "  RF.fit(X_train, y_train)\n",
    "  print(\"  -> Random Forest\")\n",
    "  y_pred = RF.predict(X_test)\n",
    "  print('       Accuracy (1 means generated data is close to real-world):', 1-accuracy_score(y_test, y_pred))\n",
    "\n",
    "  LR = LogisticRegression()\n",
    "  LR.fit(X_train, y_train)\n",
    "  print(\"  -> Logistic Regression\")\n",
    "  y_pred = LR.predict(X_test)\n",
    "  print('       Accuracy:', 1-accuracy_score(y_test, y_pred))\n",
    "\n",
    "  NN = MLPClassifier(solver='adam', hidden_layer_sizes=(150, 100, 50), max_iter=300,activation='relu')\n",
    "  NN.fit(X_train, y_train)\n",
    "  print(\"  -> Neural Network\")\n",
    "  y_pred = NN.predict(X_test)\n",
    "  print('       Accuracy:', 1-accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Adult Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('adult')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna().reset_index(drop=True) # train has nan values in \"workclass\", \"occupation\", and \"native-country\"\n",
    "test = test.dropna().reset_index(drop=True) # test has nan values in \"workclass\", \"occupation\", and \"native-country\"\n",
    "# generated_great = generated_great.replace({ # generated_great had values not in real-data\n",
    "#   'workclass': {'Self-emp-not': 5, 'Self-emp-not-': 5,'Self-emp-': 4, 'Self-': 4, 'Local-': 1, 'Self-emp': 4},\n",
    "#   'marital-status': {'Married-civ-sp': 2, 'Married-civ-': 2, 'Married-civ': 2, 'Divor': 0, 'Married-': 1, 'Never': 4, 'Married-c': 2, 'Separ': 5, 'Div': 0, 'Never-': 4, 'Married-spouse-': 3, 'Married-spspouse-absent': 3, 'Not-in-family': np.nan},\n",
    "#   'education': {'Some-': 15, '10': 0, 'B': 9, 'HS': 11, 'HS-': 11, 'Doctor': 10, 'Prof-': 15, 'Assoc-ac': 7, 'Bachel': 9, '7688': np.nan, '7th-': 5, 'Assoc': 7, 'Prof': 14, '9': 6, '7': 5, 'Some': 15, '1st-': 3},\n",
    "#   'occupation': {'Machine-op-ins': 6, 'Prof-special': 9, 'Machine-op-insp': 6, 'Machine-op-': 6, 'Exec-manager': 3, 'Farming-': 4, 'Craft': 2, 'Adm-cler': 0, 'Machine-op': 6, 'Exec': 3, 'Other-': 7, 'Farming-f': 4, 'Handlers-': 5, 'Transport-': 6, 'Machine': 6, 'Adm-': 0, 'Exec-': 3, 'Adm': 0, 'Craft-': 2, 'Farming': 4, 'Ad': 0, 'Prof-': 9, 'Prof': 9},\n",
    "#   'relationship': {'Hus': 0, 'Own': 3, 'Not-in-': 1, 'Not-in': 1, 'Own-': 3},\n",
    "#   'race': {'Amer-Indian-E': 0, 'Asian': 1, 'Asian-Pac-Is': 1, 'Asian-Pac-': 1, 'Asian-': 1, 'Asian-Pac': 1, 'Jamaica': 3},\n",
    "#   'income': {'>': 1, '<=': 0, '<=50': 0, '>50': 1}\n",
    "# })\n",
    "generated_great = generated_great.dropna().reset_index(drop=True) # generated_great has nan values in 'workclass', 'occupation', 'education', 'marital-status'\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "# generated_paft = generated_paft.replace({\n",
    "#   'workclass': {'State-service': np.nan},\n",
    "#   'education': {'6th-8th': np.nan},\n",
    "#   'marital-status': {'Private': np.nan},\n",
    "#   'occupation': {'Own-child': np.nan, 'Othermaiden': np.nan, 'Iran': np.nan, 'Prof-specialty deals': 9}\n",
    "# })\n",
    "generated_paft = generated_paft.dropna().reset_index(drop=True) # generated_great has nan values in 'workclass', 'occupation', 'native-country', 'marital-status'\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'income', label_col_discrete=True)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'income', label_col_discrete=True)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'income', label_col_discrete=True)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge check (education <-> education-num)\n",
    "education_dict = dict(zip(train['education'], train['education-num']))\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(test['education'])):\n",
    "  if education_dict[test['education'][i]] != test['education-num'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(test['education'])*100} error (%) in test.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_great['education'])):\n",
    "  if education_dict[generated_great['education'][i]] != generated_great['education-num'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_great['education'])*100} error (%) in generated_great.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_paft['education'])):\n",
    "  if education_dict[generated_paft['education'][i]] != generated_paft['education-num'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_paft['education'])*100} error (%) in generated_paft.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Adult Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('california_housing')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n-------------------- California Housing Dataset ---------------------\")\n",
    "\n",
    "train = train.dropna().reset_index(drop=True) # train has nan values in \"total_bedrooms\"\n",
    "test = test.dropna().reset_index(drop=True) # test has nan values in \"total_bedrooms\"\n",
    "generated_great = generated_great.dropna().reset_index(drop=True) # generated_great has nan values in \"total_bedrooms\"\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "# generated_paft['ocean_proximity'] = generated_paft['ocean_proximity'].replace({'NEAR OCEAN OCEAN': 4, 'NEAR BAY BAY': 3, 'is <1H OCEAN': 0}) # generated_paft had values not in real-data\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'median_house_value', label_col_discrete=False)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge check (long, lat <-> in CA?)\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "test_tmp = test[(test['longitude']<=-110) & (test['longitude']>=-130) & (test['latitude']>=30) & (test['latitude']<=45)].copy()\n",
    "geometry = [Point(xy) for xy in zip(test_tmp['longitude'], test_tmp['latitude'])]\n",
    "gdf = GeoDataFrame(test_tmp, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(\"./US_map/cb_2018_us_state_20m.shp\")\n",
    "world = world.to_crs(\"EPSG:4326\")\n",
    "world['STUSPS'].unique()\n",
    "\n",
    "ax=world.loc[world['STUSPS'] == 'CA'].plot(figsize=(15, 15), alpha=0.2)\n",
    "gdf.plot(ax=ax, marker='o', color='yellow', markersize=1,legend=True)\n",
    "\n",
    "generated_great_tmp = generated_great[(generated_great['longitude']<=-110) & (generated_great['longitude']>=-130) & (generated_great['latitude']>=30) & (generated_great['latitude']<=45)].copy()\n",
    "geometry = [Point(xy) for xy in zip(generated_great_tmp['longitude'], generated_great_tmp['latitude'])]\n",
    "gdf = GeoDataFrame(generated_great_tmp, geometry=geometry)\n",
    "gdf.plot(ax=ax, marker='^', color='blue', markersize=25,legend=True)\n",
    "\n",
    "generated_paft_tmp = generated_paft[(generated_paft['longitude']<=-110) & (generated_paft['longitude']>=-130) & (generated_paft['latitude']>=30) & (generated_paft['latitude']<=45)].copy()\n",
    "geometry = [Point(xy) for xy in zip(generated_paft_tmp['longitude'], generated_paft_tmp['latitude'])]\n",
    "gdf = GeoDataFrame(generated_paft_tmp, geometry=geometry)\n",
    "gdf.plot(ax=ax, marker='x', color='red', markersize=25, legend=True)\n",
    "\n",
    "ax.legend(['Original', 'Generated - GReaT', 'Generated - PAFT'], loc='upper right') \n",
    "\n",
    "ax.set_title(\"Houses in California\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US Geo-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Geo Location Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('geo_bird_latlevel')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n-------------------- Geo Bird Dataset ---------------------\")\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'bird', label_col_discrete=True)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'bird', label_col_discrete=True)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'bird', label_col_discrete=True)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge check (state_code <-> bird)\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('geo_bird_latlevel')\n",
    "\n",
    "bird_dicts = dict(zip(train['state_code'], train['bird']))\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(test['state_code'])):\n",
    "  if bird_dicts[test['state_code'][i]] != test['bird'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(test['state_code'])*100} error (%) in test.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_great['state_code'])):\n",
    "  if bird_dicts[generated_great['state_code'][i]] != generated_great['bird'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_great['state_code'])*100} error (%) in test.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_paft['state_code'])):\n",
    "  if bird_dicts[generated_paft['state_code'][i]] != generated_paft['bird'][i]:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_paft['state_code'])*100} error (%) in test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bejing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Bejing Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('bejing')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n-------------------- Bejing Dataset ---------------------\")\n",
    "\n",
    "train['pm2.5'].fillna(-1, inplace=True)\n",
    "test['pm2.5'].fillna(-1, inplace=True)\n",
    "generated_great['pm2.5'].fillna(-1, inplace=True)\n",
    "# generated_paft = generated_paft.replace({'cbwd': {'cv.8': 3}})\n",
    "\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'pm2.5', label_col_discrete=False)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'pm2.5', label_col_discrete=False)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'pm2.5', label_col_discrete=False)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seattle Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Seattle Housing Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('seattle_housing')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  if col not in generated_paft: continue\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n-------------------- Seattle Housing Dataset ---------------------\")\n",
    "\n",
    "train.drop(columns=['lot_size', 'lot_size_units'], inplace=True)\n",
    "test.drop(columns=['lot_size', 'lot_size_units'], inplace=True)\n",
    "generated_great.drop(columns=['lot_size', 'lot_size_units'], inplace=True)\n",
    "# generated_great = generated_great.replace({'size_units': {'779.0': 0}})\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'price', label_col_discrete=False)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'price', label_col_discrete=False)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'price', label_col_discrete=False)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge check (state_code <-> bird)\n",
    "\n",
    "zip_code = range(98033, 98199+1)\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(test['zip_code'])):\n",
    "  if test['zip_code'][i] not in zip_code:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(test['zip_code'])*100} error (%) in test.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_great['zip_code'])):\n",
    "  if generated_great['zip_code'][i] not in zip_code:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_great['zip_code'])*100} error (%) in great.\")\n",
    "\n",
    "wrong = 0\n",
    "for i in range(len(generated_paft['zip_code'])):\n",
    "  if generated_paft['zip_code'][i] not in zip_code:\n",
    "    wrong += 1\n",
    "print(f\"{wrong/len(generated_paft['zip_code'])*100} error (%) in paft.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor class distribution\n",
    "print(f\"\\n-------------------- Travel Dataset ---------------------\")\n",
    "train, test, generated_great, generated_paft, label_dictionary = load_data('travel')\n",
    "\n",
    "print(\"Test dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(test[col])))\n",
    "  extra = list(set(test[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\nGreat dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_great[col])))\n",
    "  extra = list(set(generated_great[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "print(\"\\n\\npaft dataset\")\n",
    "total_lack = 0\n",
    "total_extra = 0\n",
    "for col in label_dictionary.keys():\n",
    "  if col not in generated_paft: continue\n",
    "  lack = list(set(label_dictionary[col].keys()- set(generated_paft[col])))\n",
    "  extra = list(set(generated_paft[col]) - set(label_dictionary[col].keys()))\n",
    "  if len(lack) or len(extra):\n",
    "    print(f\"In {col} -> missed: {len(lack)} values, added: {len(extra)} values.\")\n",
    "  total_lack += len(lack)\n",
    "  total_extra += len(extra)\n",
    "if total_lack or total_extra:\n",
    "  print(f\"=> Total missed: {total_lack} values, added: {total_extra} values.\")\n",
    "\n",
    "train.replace(label_dictionary, inplace=True)\n",
    "test.replace(label_dictionary, inplace=True)\n",
    "generated_great.replace(label_dictionary, inplace=True)\n",
    "generated_paft.replace(label_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n-------------------- Travel Dataset ---------------------\")\n",
    "\n",
    "train.drop(columns=['FrequentFlyer'], inplace=True)\n",
    "test.drop(columns=['FrequentFlyer'], inplace=True)\n",
    "generated_great.drop(columns=['FrequentFlyer'], inplace=True)\n",
    "generated_paft = generated_paft[list(train.columns)] # re-order generated_paft's columns\n",
    "\n",
    "print(\"\\nMachine Learning Effienciency:\")\n",
    "print(\" - Real data:\")\n",
    "MLE(train, test, 'Target', label_col_discrete=True)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "MLE(generated_great, test, 'Target', label_col_discrete=True)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "MLE(generated_paft, test, 'Target', label_col_discrete=True)\n",
    "\n",
    "print(\"\\nDiscriminator\")\n",
    "# print(\" - Real data:\")\n",
    "# Discriminator(train, test, test)\n",
    "\n",
    "print(\" - Great method:\")\n",
    "Discriminator(train, test, generated_great)\n",
    "\n",
    "print(\" - paft method:\")\n",
    "Discriminator(train, test, generated_paft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_gpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
